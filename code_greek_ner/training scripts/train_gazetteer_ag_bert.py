# -*- coding: utf-8 -*-
"""

Automatically generated by Colab.

Original file is located at
"""
# clone the glaux nlp repo

# clone greek_ner repo for the data 

import sys
sys.path.insert(1, 'glaux-nlp-fork')

from classification.Classifier import Classifier

base_output_dir = 'gazetteer_incorporation/'

train_path = 'greek_ner_data/training_with_fancy/train.conll'
val_path = 'greek_ner_data/training_with_fancy/val.conll'
test_path = 'greek_ner_data/training_with_fancy/test.conll'


clrf = Classifier('pranaydeeps/Ancient-Greek-BERT',base_output_dir + '/AG_BERT-reduced_with_gazetteer_FINAL',
                  tokenizer_path='pranaydeeps/Ancient-Greek-BERT',
                  training_data= train_path,
                  eval_data=val_path,
                  test_data=test_path,
                  ignore_label='O',
                  data_preset='simple',
                  feature_cols={'ID':0, 'FORM':1, 'LEMMA':2, 'MISC':3, 'MASK':4},
                  add_prefix_space=True)

from tokenization import Tokenization
from data.Datasets import build_dataset

def tokenize_sentence(sentence, tokenizer, return_tensors=None, max_length=512):
    encodings = tokenizer(sentence['tokens'], truncation=True, max_length=max_length, is_split_into_words=True, return_offsets_mapping=True,return_tensors=return_tensors, return_token_type_ids=False)
    encodings['subword_ids'] = encodings.word_ids()
    return encodings

wids, train_tokens, train_tags = clrf.reader.read_tags('MISC', clrf.training_data, return_wids=True)
_, mask = clrf.reader.read_tags('MASK', clrf.training_data, return_wids=False)

def nested_Lst_str_to_int(nested_List):
    new_nested = []

    for lst in nested_List:
        sublst = []
        for item in lst:
            sublst.append(int(item))
        new_nested.append(sublst)
    assert len(nested_List) == len(new_nested)

    return new_nested

train_tag_dict = {'MISC':train_tags,
                  'token_type_ids': nested_Lst_str_to_int(mask)}
print(train_tag_dict['MISC'][0])

train_tokens_norm = Tokenization.normalize_tokens(train_tokens, 'NFC')
tag2id, id2tag = clrf.id_label_mappings(train_tags)
training_data = build_dataset(train_tokens_norm, train_tag_dict, wids)
training_data = training_data.map(tokenize_sentence,fn_kwargs={"tokenizer":clrf.tokenizer})
# training_data = training_data.map(lambda x: x.update({'token_type_ids':x['token_type_ids_true']}))
print(training_data[0])
training_data = training_data.map(clrf.align_labels, fn_kwargs={"tag2id":tag2id})
training_data = training_data.map(clrf.align_token_type_ids)
print(training_data[0])

# training_data.save_to_disk('ran_gazetteer_reduced/train')
# from datasets import load_from_disk
# training_data = load_from_disk('fancy/ran_gazetteer_reduced/train')

from tokenization import Tokenization
from data.Datasets import build_dataset

wids, eval_tokens, eval_tags = clrf.reader.read_tags('MISC', clrf.eval_data, return_wids=True)
_, eval_mask = clrf.reader.read_tags('MASK', clrf.eval_data, return_wids=False)

eval_tag_dict = {'MISC':eval_tags,
                  'token_type_ids': nested_Lst_str_to_int(eval_mask)}

print(eval_tag_dict['MISC'][0])

eval_tokens_norm = Tokenization.normalize_tokens(eval_tokens, 'NFC')
tag2id, id2tag = clrf.id_label_mappings(eval_tags)
eval_data = build_dataset(eval_tokens_norm, eval_tag_dict, wids)
print(eval_data[0])
eval_data = eval_data.map(tokenize_sentence,fn_kwargs={"tokenizer":clrf.tokenizer})
eval_data = eval_data.map(clrf.align_labels, fn_kwargs={"tag2id":tag2id})
eval_data = eval_data.map(clrf.align_token_type_ids)

from transformers import AutoTokenizer, AutoModelForTokenClassification, AutoConfig, TrainingArguments, Trainer, DataCollatorForTokenClassification, IntervalStrategy, EarlyStoppingCallback, set_seed



#for reproducibility:

set_seed(1234)

training_args = TrainingArguments(output_dir=clrf.model_dir,
                                  num_train_epochs=6,
                                  per_device_train_batch_size=16,
                                  per_device_eval_batch_size=16,
                                  learning_rate=1.2627092937313564e-05,
                                  save_strategy= IntervalStrategy.STEPS,
                                  evaluation_strategy = IntervalStrategy.STEPS, # "steps"
                                  eval_steps = 500, # Evaluation and Save happens every 500 steps, also default for when training loss is
                                  #reported so seemed appropriate
                                  save_total_limit = 1, # Only last 3 models are saved. Older ones are deleted.
                                  weight_decay=0.01,
                                  push_to_hub=False,
                                  metric_for_best_model = 'f1',
                                  load_best_model_at_end=True,
                                  warmup_ratio = 0.1,
                                  )

clrf.train_classifier(output_model=clrf.model_dir, train_dataset=training_data ,eval_dataset=eval_data,tag2id=tag2id,id2tag=id2tag, training_args=training_args, resume_from_checkpoint=True)

wids, test_tokens, test_tags = clrf.reader.read_tags('MISC', clrf.test_data, return_wids=True)
_, test_mask = clrf.reader.read_tags('MASK', clrf.test_data, return_wids=False)

print(test_mask[24])

test_tag_dict = {'MISC':test_tags,
                  'token_type_ids': nested_Lst_str_to_int(test_mask)}

print(test_tag_dict['MISC'][0])

test_tokens_norm = Tokenization.normalize_tokens(test_tokens, 'NFC')
tag2id, id2tag = clrf.id_label_mappings(test_tags)
test_data = build_dataset(test_tokens_norm, test_tag_dict, wids)
print(test_data[24])

test_data = test_data.map(tokenize_sentence,fn_kwargs={"tokenizer":clrf.tokenizer})
print(test_data[24])

test_data = test_data.map(clrf.align_labels, fn_kwargs={"tag2id":tag2id})
test_data = test_data.map(clrf.align_token_type_ids)
print(test_data[24])

prediction = clrf.predict(test_data, clrf.model_dir ,batch_size=16)
clrf.write_prediction(wids,test_tokens,test_tags,prediction,'gazetteer_incorporation/AG_BERT-reduced_with_gazetteer_glaux.conll','simple')